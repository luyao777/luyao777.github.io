[{"section":"Blog","slug":"/blog/post-7/","title":"论文解读 Explicit Feature Interaction-aware Uplift Network for Online Marketing","description":"this is meta description","date":"March 27, 2024","image":null,"imageSM":null,"searchKeyword":"","categories":"论文解读, 推荐算法","tags":"论文解读, 推荐算法","content":"Explicit Feature Interaction-aware Uplift Network for Online Marketing\n"},{"section":"Blog","slug":"/blog/post-6/","title":"论文解读 DIN: Deep Interest Network for Click-Through Rate Prediction","description":"this is meta description","date":"March 25, 2024","image":null,"imageSM":null,"searchKeyword":"","categories":"论文解读, 推荐算法","tags":"论文解读, 推荐算法","content":"arxiv: https://arxiv.org/abs/1706.06978 Github: https://github.com/mouna99/dien/tree/master 摘要 DIN是阿里发表在 KDD2018 上的一篇关于对用户行为序列建模的一篇论文。论文出了一种新型模型： 深度兴趣网络（DIN）。通过设计一个局部激活单元来自适应地学习用户对某一广告历史行为的兴趣表征，从而解决了这一难题。这种表征向量会随不同的广告而变化，从而大大提高了模型的表达能力。此外，论文还提出了迷你批量感知正则化和数据自适应激活函数，它们可以帮助训练具有数亿个参数的工业深度网络。DIN 目前已成功应用于阿里巴巴的在线展示广告系统，为主要流量提供服务。\n所以这篇论文注意就是以下几个创新点：\n局部激活单元,可从给定广告的历史行为中自适应地学习用户兴趣的表示。 迷你批量感知正则化器，它可以节省参数数量巨大的深度网络正则化的繁重计算，并有助于避免过拟合； 数据自适应激活函数，它通过考虑输入的分布来泛化 PReLU，并显示出良好的性能。 DEEP INTEREST NETWORK 论文当中使用了注意力方法来建模短期局部的兴趣。 论文当中的注意力方法与传统注意力方法不同的是，注意力权重 $$\\sum_{i}w_{i}= 1$$ 的约束条件被放宽，目的是保留用户兴趣的强度。\n也就是说，放弃了对输出进行 softmax 归一化。取而代之的是，权重的大小被视为被激活用户兴趣强度的近似值。传统的注意力方法通过对输出进行归一化处理，失去了对候选商品的注意力分辨率的体现。\nMini-batch Aware Regularization 迷你批次的正则化主要是用来处理过拟合的问题，如果没有正则，模型的表现会在第一个 epoch之后下降非常快。\n论文提出了 Mini-batch Aware Regularization，它只计算每个迷你批量中出现的稀疏特征参数的 L2 值，并使计算成为可能。事实上，正是 embedding 字典贡献了 CTR 网络的大部分参数，并造成了繁重计算的困难。\n$\\mathbf{W}$ 表示所有 embedding 的参数，施加在参数上的 L2 正则表达式可以表示为：\n$$L_2(\\mathbf{W})=|\\mathbf{W}|_2^2=\\sum_{j=1}^K|\\mathbf{w}_j|_2^2=\\sum_{(x,y)\\in\\mathcal{S}}\\sum_{j=1}^K\\frac{I(\\boldsymbol{x}_j\\neq0)}{\\boldsymbol{n}_j}|\\boldsymbol{w}_j|_2^2,$$\n如果以 mini-batch 的形式，公式又可以写成如下形式：\n$$L_2(\\mathbf{W})=\\sum_{j=1}^K\\sum_{m=1}^B\\sum_{(x,y)\\in\\mathcal{B}_m}\\frac{I(x_j\\neq0)}{n_j}|w_j|_2^2$$\n那么，在每个 mini-batch 的的参数 L2 正则化后的梯度变化为：\n$$w_j\\leftarrow w_j-\\eta\\left[\\frac1{|B_m|}\\sum_{(x,y)\\in\\mathcal{B}_m}\\frac{\\partial L(p(x),y)}{\\partial w_j}+\\lambda\\frac{\\alpha_{mj}}{n_j}w_j\\right]$$\nData Adaptive Activation Function 论文认为 PReLU 取值为 0 的硬校正点，这可能不适合各层输入分布不同的情况。考虑到这一点，论文设计了一种新的数据自适应激活函数 Dice.\n$$f(s)=p(s)\\cdot s+(1-p(s))\\cdot\\alpha s,\\quad p(s)=\\frac1{1+e^{-\\frac{s-E[s]}{\\sqrt{Var[s]+\\epsilon}}}}$$\nDice 也是 0-1 之间的激活函数，在 0-1 之间平滑过渡，如下图所示\nDice 可以被看作是 PReLu 的泛化。Dice 的主要思想是根据输入数据的分布自适应地调整，其值被设定为输入数据的平均值。此外，Dice 还能控制两个通道之间的平滑切换。当 $E(s) = 0$ 和 $Var[s] = 0$ 时，Dice 退化为 PReLU。\n评估指标 论文里面还介绍了几种比较有趣的评估指标 一种是加权 AUC，它通过对用户的 AUC 取平均值来衡量用用户内部商品顺序的好坏，并被证明与显示广告系统的在线性能更为相关。\n这里有点像是 GAUC 的概念。\n$$\\mathrm{AUC}=\\frac{\\sum_{i=1}^n impression_i\\times\\mathrm{AUC}_i}{\\sum_{i=1}^n impression_i}$$\n另外一个就是相关曝光指标 RelaImpr，用来衡量模型的提升程度，这个看看就好，直接与 $auc=0.5$ 相比较也差不多。\n$$RelaI\\boldsymbol{mpr}=\\left(\\frac{\\mathrm{AUC}(\\text{measured model)}-0.5}{\\mathrm{AUC}(\\mathrm{base~model})-0.5}-1\\right)\\times100%.$$\n相关代码 Dice 实现的 python 代码：\nimport tensorflow as tf def dice(_x, axis=-1, epsilon=0.000000001, name=\u0026#39;\u0026#39;): with tf.variable_scope(name, reuse=tf.AUTO_REUSE): alphas = tf.get_variable(\u0026#39;alpha\u0026#39;+name, _x.get_shape()[-1], initializer=tf.constant_initializer(0.0), dtype=tf.float32) input_shape = list(_x.get_shape()) reduction_axes = list(range(len(input_shape))) del reduction_axes[axis] broadcast_shape = [1] * len(input_shape) broadcast_shape[axis] = input_shape[axis] # case: train mode (uses stats of the current batch) mean = tf.reduce_mean(_x, axis=reduction_axes) brodcast_mean = tf.reshape(mean, broadcast_shape) std = tf.reduce_mean(tf.square(_x - brodcast_mean) + epsilon, axis=reduction_axes) std = tf.sqrt(std) brodcast_std = tf.reshape(std, broadcast_shape) x_normed = (_x - brodcast_mean) / (brodcast_std + epsilon) # x_normed = tf.layers.batch_normalization(_x, center=False, scale=False) x_p = tf.sigmoid(x_normed) return alphas * (1.0 - x_p) * _x + x_p * _x def parametric_relu(_x): alphas = tf.get_variable(\u0026#39;alpha\u0026#39;, _x.get_shape()[-1], initializer=tf.constant_initializer(0.0), dtype=tf.float32) pos = tf.nn.relu(_x) neg = alphas * (_x - abs(_x)) * 0.5 return pos + neg "},{"section":"Blog","slug":"/blog/post-5/","title":"MLIR 优化学习","description":"this is meta description","date":"March 20, 2024","image":null,"imageSM":null,"searchKeyword":"","categories":"Tensorflow, MLIR","tags":"Tensorflow, MLIR","content":"MLIR 简介 MLIR（多级中间表示）是编译器实用工具的表示格式和库，它位于模型表示与生成硬件特定代码的低级编译器/执行器之间。\nMLIR 本质上是用于现代优化编译器的灵活基础架构。这意味着它由一个中间表示 (IR) 规范和一个用于对该表示执行转换的代码工具包组成。（用编译器领域的话来说，当您从高级表示转换为低级表示时，此类转换可以被称为“降级”。）\nMLIR 深受 LLVM 的影响，并且明显重用了后者的许多绝佳创意。它有一个灵活的类型系统，并允许在同一编译单元中结合多个级别的抽象来表示、分析和转换计算图。这些抽象包括 TensorFlow 运算、嵌套多面体循环区域，甚至包括 LLVM 指令和固定硬件运算与类型。\n我们希望 MLIR 能引起许多群体的关注，其中包括：\n希望优化机器学习模型的性能与内存消耗的编译器研究员和实现者 正在想办法将自己的硬件（例如 TPU、手机中可移植的神经网络硬件以及其他自定义专用集成电路 (ASIC)）连接至 TensorFlow 的硬件制造商 在编写语言绑定时想利用优化编译器和硬件加速的人士 TensorFlow 生态系统包含许多在软件和硬件堆栈的多个级别上运行的编译器和优化器。我们希望逐步采用 MLIR 来简化此堆栈的各个方面。\n这里引用一下官网的介绍：\nMLIR is intended to be a hybrid IR which can support multiple different requirements in a unified infrastructure. For example, this includes:\nThe ability to represent dataflow graphs (such as in TensorFlow), including dynamic shapes, the user-extensible op ecosystem, TensorFlow variables, etc. Optimizations and transformations typically done on such graphs (e.g. in Grappler). Ability to host high-performance-computing-style loop optimizations across kernels (fusion, loop interchange, tiling, etc.), and to transform memory layouts of data. Code generation “lowering” transformations such as DMA insertion, explicit cache management, memory tiling, and vectorization for 1D and 2D register architectures. Ability to represent target-specific operations, e.g. accelerator-specific high-level operations. Quantization and other graph transformations done on a Deep-Learning graph. Polyhedral primitives. Hardware Synthesis Tools / HLS. 架构分析中关注三个方面的表达，分别是计算架构(Computation Element)，存储结构(Memory Hierarchy )和互联结构(Interconnect)。\n数据流 根据定义，AI的数据流可以分为三类，输出静止(Output Stationary)，权重静止(Weight Stationary)和行静止(Row Stationary)。\n1. 输出静止（Output Stationary） 在输出静止中，输入数据在每一层神经元之间传输，而权重参数会被复制到每个神经元中，因此权重是随着输入数据一起移动的。 这意味着神经网络的每个节点（神经元）有自己的权重参数，并且这些权重参数在整个网络的训练和推理过程中不发生改变。\n2. 权重静止（Weight Stationary） 在权重静止中，权重参数在所有神经元之间共享，在数据流过程中保持不变，而输入数据会根据不同的神经元进行移动。 这种方式下，权重参数对于整个神经网络都是固定的，而输入数据会根据不同的节点进行处理，权重参数不会随着数据移动而改变。\n3. 行静止（Row Stationary） 在行静止中，输入数据在神经网络的每一层之间传输，而权重参数则按行（或列）进行共享，即每个神经元共享一行（或列）的权重。\n这种方式允许在神经网络中共享权重参数，减少了参数数量，同时提高了计算效率。 这些不同的数据流模式影响了神经网络的训练速度、内存占用和计算效率。选择适合任务需求的数据流模式可以优化神经网络的性能。在实际应用中，根据具体的情况来选择合适的数据流模式是非常重要的。\nGEMM 编译优化 1. 矩阵分块（Tile） 当前的处理器性能主要受限于内存墙，即计算速度要大于数据存储的速度。为了打破内存墙的约束，各类硬件包括CPU及其他专用处理器，会设置不同层次的存储单元，而这些不同层级的存储单元往往大小以及读写速度不同，一般越靠近计算单元的存储其存储容量也越小但访问的速度也越快。如果可以将计算过程的数据局部化分块，而这些分块的数据可以独立完成计算，那么分块的数据就可以放在层次化的存储中，然后通过不同存储间建立Ping-Pong的数据传输方式，将数据存储与计算解耦，从而可以有效得隐藏存储墙的问题，提高计算效率。矩阵运算就有这种特点，因而可以通过矩阵分块来加速运算。\n2. 向量化（Vectorize） 向量化的操作，主要是利用硬件的向量化指令或者SIMD（单指令多数）指令的特性，实现一个指令周期对多个值操作的能力。如下图2所示，通过将4个数据组成向量，利用处理器可以处理4个元素的新向量的计算能力，可以将4个指令周期的处理时间，压缩成1个指令周期的处理时间，从而极大提高运算处理能力。\n3. 循环展开（Unroll） 由于矩阵乘法有多层循环构成，如果底层硬件有一定的并行化能力，包括多线程多进程处理能力等，那么可以对循环进行适当展开，从而提高计算的并行度，增加并发执行的机会。比如将一个次数为1024的循环，展开成256次循环，新的循环内又包含4条可以并行执行的展开计算，如果处理器能够并行处理循环内部的展开计算，那么通过对原来的循环展开，可以获得接近4倍的性能提升。\nMLIR 相关项目 tensorflow：没有tf就没有MLIR mhlo：tensorflow组件，相当于支持动态规模的XLA tfrt：tensorflow组件，tf新的runtime torch-mlir：连接pytorch与mlir生态 onnx-mlir：连接onnx与mlir生态 iree：深度学习end2end编译器 circt：硬件设计及软硬件协同开发 flang：FORTRAN的编译器前端 polygeist：C/C++ source code变成mlir Affine "},{"section":"Blog","slug":"/blog/post-4/","title":"tensorflow 异步训练及其优化","description":"this is meta description","date":"March 18, 2024","image":null,"imageSM":null,"searchKeyword":"","categories":"Tensorflow, 分布式","tags":"Tensorflow, 分布式","content":"目前遇到了 tensorflow 进行分布式训练中出现 worker 训练不均的情况，这里记录一下解决问题查找的一些资料和想法推测。\n关于分布式的原理以及源码说明，可以参考最后的“主要参考资料”部分。\n目前框架实现部分使用了 ParameterServerStrategy 分布式策略，主要对应的文章就是 TensorFlow 分布式之 ParameterServerStrategy V1 。\n该算法默认采用了 Round-Robin算法 , 链接当中有说明，那我们看一下这个调度算法是否会造成 worker分布不均的情况。\nRound-Robin Scheduling 参考：wiki 时间片轮转调度(Round-Robin Scheduling) 是进程和网络调度程序常用的算法之一。这一方法将相等长度的时间片按照不变的顺序依次分配给每个进程[2]，且在处理所有进程时不考虑任何优先级。这一算法简单并易于实现，并且不会产生饥饿问题。时间片轮转调度可以应用于其他调度问题，例如计算机网络中的数据包调度。它是一个操作系统概念。\n该算法的名称来自于其他领域通用的循环制原则，即每个参与者轮流获得相同分量的物品。\n为了公平地调度进程，循环调度程序通常采用分时机制，为每个作业分配一个时间片或时间量（CPU 时间），如果用完这一分配的时间还没有完成，则中断该进程。下次为该进程分配时间时，该进程将恢复执行。如果进程在其时间片内终止或将其状态更改为等待（或阻塞），则调度程序会选择就绪队列中的第一个进程来执行。\n循环算法是一种抢占式算法，因为一旦时间片用尽，调度程序就会强制性的暂停进程的执行。\n例如，如果时间片为100毫秒，而进程1完成的总时间为250毫秒，则循环调度程序将在100毫秒后暂停该进程，并让其他进程在CPU上占用时间。一旦其他线程都使用过一次相同的时间片（100毫秒），进程1将获得另一次CPU时间分配。这个过程一直将持续循环到进程结束并且不需要更多的CPU时间。\n美团技术团队也有一篇文章提到了这部分的优化，TensorFlow在推荐系统中的分布式训练优化实践 ，这篇文章是这么理解的：\n这部分优化，是分布式计算的经典优化方向。PS架构是一个典型的“水桶模型”，为了完成一步训练，Worker端需要和所有PS完成交互，因此PS之间的平衡就显得非常重要。但是在实践中，我们发现多个PS的耗时并不均衡，其中的原因，既包括TensorFlow PS架构简单的切图逻辑（Round-Robin）带来的负载不均衡，也有异构机器导致的不均衡。 对于推荐模型来说，我们的主要优化策略是，把所有稀疏参数和大的稠密参数自动、均匀的切分到每个PS上，可以解决大多数这类问题。而在实践过程中，我们也发现一个比较难排查的问题：原生Adam优化器，实现导致PS负载不均衡。下面会详细介绍一下。在Adam优化器中，它的参数优化过程需要两个β参与计算，在原生TensorFlow的实现中，这两个β是所有需要此优化器进行优化的Variabl（或HashTable）所共享的，并且会与第一个Variable（名字字典序）落在同一个PS上面，这会带来一个问题：每个优化器只拥有一个β_1和一个β_2，且仅位于某个PS上。因此，在参数优化的过程中，该PS会承受远高于其他PS的请求，从而导致该PS成为性能瓶颈。\n所以，这里的 round-robin 调度算法极有可能造成 worker 之间利用率不均匀的情况。要解决这个问题，\n可以更换调度算法 对优化器部分进行优化 Dataset 与 变量分片 这里引用一下 tensorflow 当中对变量分片的解释：\n变量分片是指将一个变量拆分为多个较小的变量，这些变量称为分片。在访问这些分片时，变量分片可能有助于分配网络负载。这对在多个参数服务器之间分布计算和存储普通变量也很有用，例如，当使用可能不适合单个机器内存的非常大的嵌入时。\n要启用变量分片，您可以在构造 ParameterServerStrategy 对象时传入 variable_partitioner。每次创建变量时都会调用 variable_partitioner，它预计会返回该变量每个维度上的分片数。提供了一些开箱即用的 variable_partitioner，例如 tf.distribute.experimental.partitioners.MinSizePartitioner。建议使用基于大小的分区程序（如 tf.distribute.experimental.partitioners.MinSizePartitioner）以避免对小变量进行分区，否则可能会对模型训练速度产生负面影响。\nsteps_per_exectuion 在模型构建过程中会使用这个函数，大意为每个 worker在执行过程中所每次执行所运算的 batch 数量，经过实践证明，这个参数调小之后会使得 worker cpu 利用率尖峰数量变多更加密集，缩短了执行时间，与参数服务器的交换速度加快。\n但是并不能解决尖峰的问题。\n这里是 keras 官方文档 解释：\nsteps_per_execution: Int. The number of batches to run during each a single compiled function call. Running multiple batches inside a single compiled function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N, Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each compiled function execution). Not supported with the PyTorch backend.\n主要参考资料 随笔分类 - 011_分布式机器学习 TensorFlow在推荐系统中的分布式训练优化实践 "},{"section":"Blog","slug":"/blog/post-3/","title":"解析 tensoflow 预热文件","description":"this is meta description","date":"February 26, 2024","image":null,"imageSM":null,"searchKeyword":"","categories":"Tensorflow","tags":"Tensorflow","content":"解析 tensoflow 预热文件 代码如下：\n注意，这里 tf_record_iterator 要使用最新的 api, 比如这里 tf2.5 使用的是 tf.compat.v1.io.tf_record_iterator。\nimport tensorflow as tf from tensorflow_serving.apis import prediction_log_pb2 def par_warmup(): i = 0 for serialized_example in tf.compat.v1.io.tf_record_iterator(\u0026#34;./assets.extra/tf_serving_warmup_requests\u0026#34;): log = prediction_log_pb2.PredictionLog() log.ParseFromString(serialized_example) print(i, type(serialized_example), type(log), \u0026#34;End\u0026#34;) i += 1 print(log) break par_warmup() 文件内容包括：\nmodel_spec inputs 形状大小 数据类型 outputs predict_log { request { model_spec { name: \u0026#34;model\u0026#34; signature_name: \u0026#34;serving_default\u0026#34; } inputs { key: \u0026#34;total_cnt\u0026#34; value { dtype: DT_INT64 tensor_shape { dim { size: 1 } dim { size: 1 } } int64_val: 1 } } } } "},{"section":"Blog","slug":"/blog/post-2/","title":"[ICLR 2023] MASKFUSION: FEATURE AUGMENTATION FOR CLICK-THROUGH RATE PREDICTION VIA INPUT-ADAPTIVE","description":"this is meta description","date":"February 7, 2024","image":null,"imageSM":null,"searchKeyword":"","categories":"论文, 推荐算法","tags":"paper, 推荐算法","content":"摘要 这篇论文提出了一个自适应特征融合框架，称为MaskFusion，以额外捕获显式的交互分析了输入特征与现有深部CTR结构之间的关系动态建模，除了现有的共同特征的相互作用的工作原理。\nMaskFusion是一种实例级别的粒度的感知，用于对提取的信息进行增强，使深度CTR模型更加个性化，通过分配每个特征与实例自适应掩码和融合每个特征与每个隐藏状态向量深部结构。MaskFusion使用起来也比较灵活，可以集成到任何现有的深度CTR模型当中。\n模型结构 核心方法 该论文的核心方法主要有三部分，接下来会分成这三部分去说明每个组件的作用。\nFusion Layer\nMask Controller\nInstance Norm Layer\nFusion Layer Fusion Layer主要的作用就是用来与NN层进行交互，作为融合的手段，论文当中选取的方式是 Concat。论文当中的解释为，如果使用element-wise的交互方式，很难对特征的可解释性有所贡献，言下之意就是concat的方式能够比较明确将增益部分做出归属判断。\n$$\\mathbf h_t=Relu(\\mathbf W_t \\mathbf{\\hat{h}}_{t-1}+\\mathbf{b}_t)$$\n$$\\hat h_t=Concate([E,Relu(W_t\\hat{h}_{t-1}+b_t)])$$\nFusion Layer 的还有提升模型的记忆能力，但是会对所有的实例级别的数据无差别的进行融合交互，忽略掉实例之间的差异。另外就是提高了记忆能力就会降低模型的泛化能力。于是，论文就设计了第二个模块，Mask Controller。\nMask Controller Mask Controller的作用在于基于每个实例的全局信息自动生成Mask，并且在Fusion Layer的融合过程中对生成的Mask进行应用。\n这里的Mask处理意味着能够融合过程中能够进行更好的记忆，我们也能够知道哪些特征能够更好地作用于CTR预测，同时意味着具有很强的可解释性。\n在论文当中，使用MLP作为演示，用来生成Mask，公式如下：\n$$\\boldsymbol\\alpha_t^k=MLP_{\\boldsymbol\\phi_t}(\\mathbf E)$$\n注意公式中的下标，下标意味着对每个实例 $k$ 都有第 $t$ 个mask生成器，生成器的个数，与即将交互的DNN的层数相同。\n为了学习哪些特征更有助于网络记忆，以及更好地进行训练收敛，论文当中采用了 $softmax$ 的激活函数对 mask 进行normalize。\n公式如下：\n$$m_{t,j}^k=\\dfrac{exp(\\alpha_{t,j}^k)}{\\sum_{t=1}^l exp(\\alpha_{t,j}^{k})},~\\forall j\\in[1,n],~\\forall t\\in[1,l]$$\nMask生成好之后，对特征向量进行相乘，然后与主网络每层的输出进行级联，送到下一层。\n$$ \\mathbf{\\hat h_t}=Concate([\\mathbf m_t^k \\mathbf E,\\mathit Relu(\\mathbf W_t \\mathbf{\\hat h_{t-1}}+\\mathbf b_t)]) $$\nInstance Norm Layer 论文中认为，Mask的效果可能被重缩放所影响，为了消除这种现象，在消失重缩放的过程中保留每一位的信息。 Batch Norm 以及 Layer Norm 都不能适用这个问题，因为BN的计算是计算一个小批量中所有实例的信息，LN的计算是计算一个实例中所有特征的信息。\n论文当中使用了实例级别的Norm，被称为Instance Norm，公式如下：\n$$IN(\\mathbf e_j^k)=\\gamma\\cdot\\dfrac{\\mathbf e_j^k-\\mu_j}{\\sqrt{\\sigma_j^2+\\epsilon}}+\\boldsymbol\\beta$$\n这里的BN LN IN的理解，可以参考这篇文章 总结 这种方式基本上属于万金油的trick优化方式，提升Mask的准确性以及泛化性，适用于各个以DNN为基础的推荐系统，也属于比较好工程实现的方式。\n"},{"section":"Blog","slug":"/blog/post-1/","title":"2024 推荐系统论文汇总","description":"this is meta description","date":"February 5, 2024","image":null,"imageSM":null,"searchKeyword":"","categories":"推荐算法, 论文","tags":"推荐, 论文","content":"对2024年的 推荐系统论文进行一波收集，给各位初学者和算法大佬作为灵感来源，后续专栏会继续更新论文解读，根据评论不断补充，欢迎大家三连~\nAAAI 2024 转载自：https://zhuanlan.zhihu.com/p/673884610\nSparse Enhanced Network: An Adversarial Generation Method for Robust Augmentation in Sequential Recommendation Backdoor Adjustment via Group Adaptation for Debiased Coupon Recommendations Temporally and Distributionally Robust Optimization for Cold-start Recommendation Learning Accurate and Bidirectional Transformation via Dynamic Embedding Transportation for Cross-Domain Recommendation Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations Peer Learning: Learning Complex Policies in Groups from Scratch via Action Recommendations CoreRec: A Counterfactual Correlation Inference for Next Set Recommendation Enhancing Job Recommendation through LLMbased Generative Adversarial Networks Temporal Graph Contrastive Learning for Sequential Recommendation Less is More: Label Recommendation for Weakly Supervised Point Cloud Semantic Segmentation Effect Size Estimation for Duration Recommendation in Online Experiments: Leveraging Hierarchical Models and Objective Utility Approaches D3: A Methodological Exploration of Domain Division, Modeling, and Balance in Multi-Domain Recommendations STEM: Unleashing the Power of Embeddings for Multi-task Recommendation No prejudice! Fair Federated Graph Neural Networks for Personalized Recommendation Fine-tuning Large Language Model based Explainable Recommendation with Explainable Quality Reward VITA: \u0026lsquo;Carefully Chosen and Weighted Less\u0026rsquo; Is Better in Medication Recommendation A Goal Interaction Graph Planning Framework for Conversational Recommendation RRL: Recommendation Reverse Learning Dual-view Whitening on Pre-trained Text Embeddings for Sequential Recommendation Graph Disentangled Contrastive Learning with Personalized Transfer for Cross-Domain Recommendation LLMRG: Improving Recommendations through Large Language Model Reasoning Graphs Spectral-based Graph Neutral Networks for Complementary Item Recommendation LGMRec: Local and Global Graph Learning for Multimodal Recommendation Plug-in Diffusion Model for Sequential Recommendation Successive POI Recommendation via Brain-inspired Spatiotemporal Aware Representation Multi-Domain Recommendation to Attract Users via Domain Preference Modeling Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential Recommendations An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention Distributional Off-Policy Evaluation for Slate Recommendations Review-Enhanced Hierarchical Contrastive Learning for Recommendation Preference Aware Dual Contrastive Learning for Item Cold-Start Recommendation Tail-STEAK: Improve Friend Recommendation for Tail Users via Self-Training Enhanced Knowledge Distillation Learning Time Slot Preferences via Mobility Tree for Next POI Recommendation Knowledge-Aware Explanable Reciprocal Recommendation Adaptive Hardness Negative Sampling for Collaborative Filtering Generalize for Future: Slow and Fast Trajectory learning for CTR prediction AT4CTR: Auxiliary Match Tasks for Enhancing Click-Through Rate Prediction WSDM 2024 转载自： https://zhuanlan.zhihu.com/p/665023987 Defense Against Model Extraction Attacks on Recommender Systems（南阳理工）【推荐系统攻防】 Sixiao Zhang (Nanyang Technological University)*; Hongzhi Yin (The University of Queensland); Hongxu Chen (The University of Queensland); Cheng Long (Nanyang Technological University)\nMotif-based Prompt Learning for Universal Cross-domain Recommendation（首都师范）【基于Motif的通用跨域推荐提示学习】 Bowen Hao (Captial Normal University)*; Chaoqun Yang (Griffith University); Lei Guo (Shandong Normal University); Junliang Yu (The University of Queesland); Hongzhi Yin (The University of Queensland)\nTo Copy, or not to Copy; That is a Critical Issue of the Output Softmax Layer in Neural Sequential Recommenders（亚马逊）【复制或不复制；这是神经序列推荐器中输出Softmax层的一个关键问题】 Haw-shiuan Chang (Amazon)*; Nikhil Agarwal (http://Amazon.com ); Andrew McCallum (Univ of Massachusetts Amherst)\nLinear Recurrent Units for Sequential Recommendation（伊利诺伊）【序列推荐的线性递归单元】 Zhenrui Yue (University of Illinois Urbana-Champaign); Yueqi Wang (University of California, Berkeley); Zhankui He (UC, San Diego)*; Huimin Zeng (University of Illinois at Urbana-Champaign); Julian McAuley (UCSD); Dong Wang (University of Illinois Urbana-Champaign)\nUser Behavior Enriched Temporal Knowledge Graph for Sequential Recommendation（新加坡国立，华为）【用户行为丰富知识图谱，用于序列推荐】 Hengchang Hu (National University of Singapore)*; Wei Guo (Huawei Noah’s Ark Lab); Xu Liu (National University of Singapore); Yong Liu (Huawei); Ruiming Tang (Huawei Noah’s Ark Lab); Rui Zhang (http://ruizhang.info ); Min-Yen Kan (National University of Singapore)\nIntent Contrastive Learning with Cross Subsequences for Sequential Recommendation（东吴大学）【基于跨子序列的意图对比学习序列推荐】 Xiuyuan Qin (Soochow University)*; Huanhuan Yuan (Soochow University); Pengpeng Zhao (Soochow University); Guanfeng Liu (Macquarie University); Fuzhen Zhuang (Institute of Artificial Intelligence, Beihang University); Victor S. Sheng (Texas Tech University)\nBudgeted Embedding Table For Recommender Systems（昆士兰）【推荐系统的嵌入表研究】 Yunke Qu (The University of Queensland)*; Tong Chen (The University of Queensland); Quoc Viet Hung Nguyen (Griffith University); Hongzhi Yin (The University of Queensland)\nPre-trained Recommender Systems: A Causal Debiasing Perspective（威斯康星，亚马逊）【预训练推荐系统：因果去偏的视角】 Ziqian Lin (University of Wisconsin–Madison)*; Hao Ding (AWS AI Lab); Nghia Trong Hoang (Washington State University); Branislav Kveton (AWS AI Labs); Anoop Deoras (Amazon); Hao Wang (Rutgers University)\nDynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation（中科大）【动态稀疏学习：一种高效推荐的新范式】 Shuyao Wang (University of Science and Technology of China)*; Yongduo Sui (University of Science and Technology of China); Jiancan Wu (University of Science and Technology of China); Zhi Zheng (University of Science and Technology of China); Hui Xiong (Hong Kong University of Science and Tech)\nPEACE: Prototype lEarning Augmented transferable framework for Cross-domain rEcommendation（蚂蚁）【PEACE：用于跨域推荐的原型lEarning增强可迁移框架】 Chunjing Gan (Ant Group)*; Bo Huang (Ant Group); Binbin Hu (Ant Group); Jian Ma (Ant Group); Zhiqiang Zhang (Ant Group); Jun Zhou (Ant Financial); Guannan Zhang (Ant Group); WENLIANG ZHONG (Ant Group)\nMADM: A Model-agnostic Denoising Module for Graph-based Social Recommendation（上交）【MADM：一个基于图的社交推荐的模型无关去噪模块】 Wenze Ma (Shanghai Jiao Tong University)*; Yuexian Wang (Shanghai Jiao Tong University); Yanmin Zhu (Shanghai Jiao Tong University); Zhaobo Wang (Shanghai Jiao Tong University); Mengyuan Jing (Shanghai Jiao Tong University); Xuhao Zhao (Shanghai Jiao Tong University); Jiadi Yu (Shanghai Jiao Tong University); Feilong Tang (Shanghai Jiao Tong University)\nCollaboration and Transition: Distilling Item Transitions into Multi-Query Self-Attention for Sequential Recommendation（蒙特利尔，快手）【协作与转换：提取项目转换为序列推荐的多查询自注意力机制】 Tianyu Zhu (University of Montreal)*; Yansong Shi (Tsinghua University); Yuan Zhang (Kuaishou Inc.); Yihong Wu (Université de Montréal); Fengran Mo (Université de Montréal); Jian-Yun Nie (Université de Montréal)\nCDRNP: Cross-Domain Recommendation to Cold-Start Users via Neural Process（中科院）【CDRNP：通过神经过程向冷启动用户提供跨领域推荐】 Xiaodong Li (Institute of Information Engineering, Chinese Academy of Sciences)*; Jiawei Sheng ( Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China); Jiangxia Cao (Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China); Tingwen Liu (Institute of Information Engineering, CAS); Wenyuan Zhang (Institute of Information Engineering, Chinese Academy of Sciences); Quangang Li (Institute of Information Engineering, CAS)\nInverse Learning with Extremely Sparse Feedback for Recommendation（卡耐基梅隆，快手）【具有极稀疏反馈的反向学习推荐】 Guanyu Lin (Carnegie Mellon University)*; Chen Gao (Tsinghua University); Yu Zheng (Tsinghua University); Yinfeng Li (Kuaishou Inc); Jianxin Chang (Kuaishou Inc); Yanan Niu (Kuaishou Inc); Yang Song (Kuaishou Technology); Kun Gai (AI); Zhiheng Li (Tsinghua University); Depeng Jin (Tsinghua University); Yong Li (Tsinghua University)\nContextual MAB Oriented Embedding Denoising for Sequential Recommendation（北邮）【面向上下文MAB的序列推荐嵌入去噪】 Zhichao Feng (Beijing University of Post and Telecommunications); Pengfei Wang (School of Computer Science, Beijing University of Posts and Telecommunications)*; Kaiyuan Li (Beijing University of Posts and Telecommunications); Chenliang Li (Wuhan University); Shangguang Wang (State Key Laboratory of Networking and Switching Technology)\nMixed Attention Network for Cross-domain Sequential Recommendation（卡耐基梅隆，快手）【跨域序列推荐的混合注意网络】 Guanyu Lin (Carnegie Mellon University)*; Chen Gao (Tsinghua University); Yu Zheng (Tsinghua University); Jianxin Chang (Kuaishou Inc); Yanan Niu (Kuaishou Inc); Yang Song (Kuaishou Technology); Kun Gai (AI); Zhiheng Li (Tsinghua University ); Depeng Jin (Tsinghua University); Yong Li (Tsinghua University); Meng Wang (Institute of Artificial Intelligence, Hefei Comprehensive National Science Center)\nKnowledge Graph Context-Enhanced Diversified Recommendation（伊利诺伊）【知识图谱上下文增强的多样化推荐】 Xiaolong Liu (University of Illinois at Chicago)*; Liangwei Yang (University of Illinois at Chicago); Zhiwei Liu (Salesforce); Mingdai Yang (University of Illinios at Chicago); Chen Wang (University of Illinois at Chicago); Hao Peng (Beihang University); Philip S Yu (UIC)\nExploring Adapter-based Transfer Learning for Recommender Systems: Empirical Studies and Practical Insights（西湖大学）【基于适配器的推荐系统迁移学习探索：实证研究与实践启示】 Junchen Fu (Westlake University)*; Fajie Yuan (Westlake University); Yu Song (Westlake University); Zheng Yuan (Westlake University); Mingyue Cheng (University of Science and Technology of China); Shenghui Cheng (Westlake University); Jiaqi Zhang (Westlake University); Jie Wang (Westlake University); Yunzhu Pan (University of Electronic Science and Technology of China)\nDiff-MSR: A Diffusion Model Enhanced Paradigm for Cold-Start Multi-Scenario Recommendation（香港城市大学，华为）【Diff-MSR：冷启动多场景推荐的扩散模型增强范式】 Yuhao Wang (City University of Hong Kong)*; Ziru Liu (City University Of HongKong ); Yichao Wang (Huawei Noah’s Ark Lab); Xiangyu Zhao (City University of Hong Kong); Bo Chen (Huawei Noah’s Ark Lab); Huifeng Guo (Huawei Noah’s Ark Lab); Ruiming Tang (Huawei Noah’s Ark Lab)\nAutoPooling: Automated Pooling Search for Multi-valued Features in Recommendations（腾讯） He Wei (Tencent Inc.)*; Meixi Liu (Machine learning platform department, Tencent TEG); Yang Zhang (Tencent Inc)\nC^2DR: Robust Cross-Domain Recommendation based on Causal Disentanglement（中南）【C^2DR：基于因果解耦的鲁棒跨域推荐】 Menglin Kong (Central South University); Jia Wang (Xi’an Jiaotong-Liverpool University)*; Yushan Pan (Xi’an Jiaotong-Liverpool University); Haiyang Zhang (Xi’an Jiaotong-Liverpool University); Muzhou Hou (Central South Uinversity) Unified Pretraining for Recommendation via Task Hypergraphs（伊利诺伊，Salesforce）【基于任务超图的推荐统一预训练】\nMingdai Yang (University of Illinios at Chicago)*; Zhiwei Liu (Salesforce); Liangwei Yang (University of Illinois at Chicago); Xiaolong Liu (University of Illinois at Chicago); Chen Wang (University of Illinois at Chicago); Hao Peng (Beihang University); Philip S Yu (UIC)\nSSLRec: A Self-Supervised Learning Library for Recommendation（港大）【自监督推荐库】 Xubin Ren (the University of Hong Kong)*; Lianghao Xia (University of Hong Kong); Yuhao Yang (Wuhan University); Wei Wei (University of Hong Kong); Tianle Wang (HKU); Xuheng Cai (The University of Hong Kong); Chao Huang (University of Hong Kong)\nMulti-Sequence Attentive User Representation Learning for Side-information Integrated Sequential Recommendation（深圳大学，腾讯）【辅助信息集成序列推荐的多序列注意用户表征学习】 Xiaolin Lin (Shenzhen University)*; Jinwei Luo (Shenzhen University); Junwei Pan (Tencent); Weike Pan (Shenzhen University); Zhong Ming (Shenzhen University); Xun Liu (Tencent); HUANG SHUDONG (tencent); Jie Jiang (Tencent Inc.)\nLabelCraft: Empowering Short Video Recommendations with Automated Label Crafting（中科大，快手）【LabelCraft：通过自动标签制作实现短视频推荐】 Yimeng Bai (University of Science and Technology of China)*; Yang Zhang (University of Science and Technology of China); Jing Lu (Kuaishou Inc); Jianxin Chang (Kuaishou Inc); Xiaoxue Zang (Kuaishou Inc); Yanan Niu (Kuaishou); Yang Song (Kuaishou Technology); Fuli Feng (University of Science and Technology of China)\nMONET: Modality-Embracing Graph Convolutional Network and Target-Aware Attention for Multimedia Recommendation（汉阳大学）【MONET：包含图卷积网络的模态和多媒体推荐的目标感知注意力】 Yungi Kim (Hanyang University); Taeri Kim (Hanyang University); Won-Yong Shin (Yonsei University, Korea); Sang-Wook Kim (Hanyang University, Korea)*\nRecJPQ: Training Large-Catalogue Sequential Recommenders【RecJPQ：训练大型目录序列推荐】 Aleksandr V Petrov (University of Glasgow); Craig Macdonald (University of Glasgow) On the Effectiveness of Unlearning in Session-Based Recommendation（山大）【基于会话的推荐中释放的有效性研究】 Xin Xin (Shandong University); Liu Yang (Shandong University); Ziqi Zhao (Shandong University); Pengjie Ren (Shandong University); Zhumin Chen (Shandong University); Jun Ma (Shandong University); Zhaochun Ren (Leiden University)\nProxy-based Item Representation for Attribute and Context-aware Recommendation（首尔国立大学）【基于代理的item表征】 Jinseok Seol (Seoul National University)*; Minseok Gang (Seoul National University); Sang-goo Lee (Seoul National University); Jaehui Park (University of Seoul)\nIncMSR: An Incremental Learning Approach for Multi-Scenario Recommendation（清华，华为）【IncMSR：一种用于多场景推荐的增量学习方法】 Kexin Zhang (Tsinghua University)*; Yichao Wang (Huawei Noah’s Ark Lab); Xiu Li (Tsinghua University); Ruiming Tang (Huawei Noah’s Ark Lab); Rui Zhang (http://ruizhang.info )\nDeep Evolutional Instant Interest Network for CTR Prediction in Trigger-Induced Recommendation（阿里）【触发推荐中CTR预测的深度进化即时兴趣网络】 Zhibo Xiao (Alibaba Group)*; Luwei Yang (Alibaba Group); Tao Zhang (Alibaba Group); Wen Jiang (Alibaba Group); Wei Ning ( Alibaba Group); Yujiu Yang (Tsinghua University)\nUser Consented Federated Recommender System Against Personalized Attribute Inference Attack Qi Hu (Hong Kong University of Science and Technology)*; Yangqiu Song (Hong Kong University of Science and Technology)\nNeural Kalman Filtering for Robust Temporal Recommendation（复旦，微软，亚马逊）【用于鲁棒时间推荐的神经卡尔曼滤波】 Jiafeng Xia (Fudan University)*; Dongsheng Li (Microsoft Research Asia); Hansu Gu (http://Amazon.com ); Tun Lu (Fudan University); Peng Zhang (Fudan University); Li Shang (Fudan University); Ning Gu (Fudan University)\nAttribute Simulation for Item Embedding Enhancement in Multi-interest Recommendation（天大）【多兴趣推荐中项目嵌入增强的属性仿真】 Yaokun Liu (Tianjin University)*; Xiaowang Zhang (Tianjin University); Minghui Zou (Tianjin University); Zhiyong Feng (Tianjin University)\nDebiasing Sequential Recommenders through Distributionally Robust Optimization over System Exposure（山大）【基于系统曝光的分布鲁棒优化对序列推荐去偏】 Jiyuan Yang (Shandong University)*; Yue Ding (Shanghai Jiao Tong University); YIDAN WANG (SHANDONG UNIVERSITY); Pengjie Ren (Shandong University); Zhumin Chen (Shandong University); Fei Cai (National University of Defense Technology); Jun Ma (Shandong University); Rui Zhang (http://ruizhang.info ); Zhaochun Ren (Leiden University); Xin Xin (Shandong University)\nKnowledge Graph Diffusion Model for Recommendation（港大）【知识图扩散模型用于推荐】 Yangqin Jiang (University of Hong Kong)*; Yuhao Yang (Wuhan University); Lianghao Xia (University of Hong Kong); Chao Huang (University of Hong Kong)\nInteract with the Explanations: Causal Debiased Explainable Recommendation System（上交，adobe）【因果去偏可解释推荐系统】 Xu Liu (Shanghai Jiao Tong University); Tong Yu (Adobe Research); Kaige Xie (Georgia Institute of Technology); Junda Wu (New York University); Shuai Li (Shanghai Jiao Tong University)*\nGlobal Heterogeneous Graph and Target Interest Denoising for Multi-behavior Sequential Recommendation（天大）【多行为序列推荐的全局异构图和目标兴趣去噪】 Xuewei Li (Tianjin University); Hongwei Chen (College of Intelligence and Computing, Tianjin University)*; Jian Yu (Tianjin University); Mankun Zhao (Tianjin University); Tianyi Xu (Tianjin University); Wenbin Zhang (Information and Network Center, Tianjin University); Mei Yu (Tianjin University) MultiFS: Automated Multi-Scenario Feature Selection in Deep Recommender Systems【MultiFS:深度推荐系统中的自动多场景特征选择】\nDugang Liu (Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen University)*; Chaohua Yang (Shenzhen University); Xing Tang (Tencent); Yejing Wang (City University of Hongkong); Fuyuan Lyu (McGill University); weihong luo (tencent); Xiuqiang He (Tencent); Zhong Ming (Shenzhen University); Xiangyu Zhao (City University of Hong Kong)\nCalibration-compatible Listwise Distillation of Privileged Features for CTR Prediction（山大，阿里）【list-wise蒸馏用于CTR预测校准】 Xiaoqiang Gui (Shandong University)*; Yueyao Cheng (Alibaba Group); Xiang-Rong Sheng (Alibaba Group); Yunfeng Zhao (Shandong University); Guoxian Yu (Shandong University); Shuguang Han (Alibaba Inc.); Yuning Jiang (Alibaba Group); Jian Xu (Alibaba Group); Bo Zheng (Alibaba Group)\nICLR 2024 转载自：https://zhuanlan.zhihu.com/p/669386030\nSTUDY: Socially Aware Temporally Causal Decoder Recommender Systems 研究：社会意识时间因果解码器推荐系统\nSUBER: An RL Environment with Simulated Human Behavior for Recommender Systems SUBER：用于推荐系统的模拟人类行为的 RL 环境\nUOEP: User-Oriented Exploration Policy for Enhancing Long-Term User Experiences in Recommender Systems UOEP：以用户为导向的探索政策，以增强推荐系统的长期用户体验\nStrategic Recommendations for Improved Outcomes in Congestion Games 改善拥堵游戏结果的战略建议\nCategorical Features of entities in Recommendation Systems Using Graph Neural Networks 使用图神经网络的推荐系统中实体的分类特征\nSafe Collaborative Filtering 安全协同过滤\nCross-domain Recommendation from Implicit Feedback 来自隐性反馈的跨领域推荐\nDisentangled Heterogeneous Collaborative Filtering\nBe Aware of the Neighborhood Effect: Modeling Selection Bias under Interference for Recommendation 注意邻域效应：在干扰下对选择偏差进行建模以进行推荐\nDemystifying Embedding Spaces using Large Language Models\nFIITED: Fine-grained embedding dimension optimization during training for recommender systems FIITED：推荐系统训练过程中的细粒度嵌入维度优化\nFrom Deterministic to Probabilistic World: Balancing Enhanced Doubly Robust Learning for Debiased Recommendation 从确定性世界到概率性世界：平衡增强型双倍鲁棒学习以实现无偏推荐\nHow Does Message Passing Improve Collaborative Filtering? 消息传递如何改进协作过滤？\nVibeSpace: Automatic vector embedding creation for arbitrary domains and mapping between them using large language models VibeSpace：使用大型语言模型为任意域自动创建向量嵌入并在它们之间进行映射\nUnifying User Preferences and Critic Opinions: A Multi-View Cross-Domain Item-sharing Recommender System 统一用户偏好和评论家意见：一个多视角的跨域物品共享推荐系统\nGNN-based Reinforcement Learning Agent for Session-based Recommendation 基于GNN的强化学习代理，用于基于会话的推荐\nBasis Function Encoding of Numerical Features in Factorization Machines for Improved Accuracy 因式分解机中数值特征的基函数编码以提高精度\nMOESART: An Effective Sampling-based Router for Sparse Mixture of Experts MOESART：一种有效的基于采样的路由器，用于稀疏专家的混合\nOn the Embedding Collapse When Scaling up Recommendation Models 关于扩展推荐模型时的嵌入崩溃\nHyperbolic Embeddings in Sequential Self-Attention for Improved Next-Item Recommendations 顺序自注意力中的双曲线嵌入，以改进下一步建议\nConstraining Non-Negative Matrix Factorization to Improve Signature Learning 约束非负矩阵分解以改善特征学习\nFarzi Data: Autoregressive Data Distillation\nFactual and Personalized Recommendation Language Modeling with Reinforcement Learning 基于强化学习的事实和个性化推荐语言建模\nConvFormer: Revisiting Token-mixers for Sequential User Modeling ConvFormer：重新审视用于顺序用户建模的令牌混合器\nTalk like a Graph: Encoding Graphs for Large Language Models 像图形一样说话：大型语言模型的编码图形\nWeight Uncertainty in Individual Treatment Effect 个体treatment效果的权重不确定性\nExplaining recommendation systems through contrapositive perturbations 通过逆向扰动解释推荐系统\nBenchmarks for Reinforcement Learning with Biased Offline Data and Imperfect Simulators 使用有偏见的离线数据和不完美的模拟器进行强化学习的基准\nEvidential Conservative Q-Learning for Dynamic Recommendations 动态推荐的证据保守 Q 学习\nUNLEARNING THE UNWANTED DATA FROM A PERSONALIZED RECOMMENDATION MODEL 从个性化推荐模型中消除不需要的数据\nAFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations AFDGCF：自适应特征去相关图协同过滤推荐\n"}]